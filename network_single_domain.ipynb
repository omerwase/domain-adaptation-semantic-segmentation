{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training for Single Domain\n",
    "\n",
    "#### Setup notes: \n",
    "    1) Data and weight file paths must be configured locally\n",
    "    2) Configuration for image preprocessing must be changed based on which dataset is used (CARLA or BDD)\n",
    "    3) CARALA test and train directory's expected structure: \n",
    "        - images in 'CameraRGB' directory \n",
    "        - labels in 'CameraSeg' directory\n",
    "    4) BDD test and train directory's expected structure:\n",
    "        - images in 'images' directory \n",
    "        - labels in 'labels' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import helper_functions_carla as hf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "from functools import reduce\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Name and Paths\n",
    "#### Note: parameter values below must be re-assigned based on local directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'network_single_domain'\n",
    "MODEL_SAVE_VER = '00'\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'datasets', 'carla')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "SAVE_DIR = os.path.join(os.getcwd(), 'saved_models', MODEL_NAME, MODEL_SAVE_VER)\n",
    "WEIGHTS_FILE = os.path.join(os.getcwd(), 'weights', 'pretrained_weights.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing Configuration\n",
    "#### Note: comment/uncomment based on which dataset is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration for CARLA Images\n",
    "TRIM = True\n",
    "TRIM_IND = (115, 523)\n",
    "FLIP = True\n",
    "RESHAPE = False\n",
    "NEW_SHAPE = (800, 408)\n",
    "LABEL_CHANNELS = [10, 7, 2, 4, 5, 8, 9, 20, 30]\n",
    "CHANNEL_NAMES = ['Back', 'Vehi', 'Road', 'Fence', 'Ped', 'Poles', 'Side', 'Veg', 'BW', 'OT']\n",
    "LOSS_WEIGHTS = [0.6, 1.2, 0.6, 1.2, 1.2, 1.2, 0.7, 0.7, 0.6, 0.8]\n",
    "\n",
    "'''\n",
    "# Configuration for BDD Images\n",
    "TRIM = False\n",
    "TRIM_IND = (0, 720)\n",
    "FLIP = True\n",
    "RESHAPE = True\n",
    "NEW_SHAPE = (640, 360)\n",
    "LABEL_CHANNELS = [13, 0, 4, 11, 5, 1, 8, 20, 30]\n",
    "CHANNEL_NAMES = ['Back', 'Vehi', 'Road', 'Fence', 'Ped', 'Poles', 'Side', 'Veg', 'BW', 'OT']\n",
    "LOSS_WEIGHTS = [0.6, 0.8, 0.6, 1.2, 1.2, 1.2, 0.8, 0.7, 0.7, 0.7]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperarameters and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "SHUFFLE_PER_EPOCH = True\n",
    "BATCH_SIZE = 12\n",
    "L2_REG = 1e-5\n",
    "STD_DEV = 1e-2\n",
    "LEARNING_RATE = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "KEEP_PROB = 0.5 \n",
    "EPSILON = 1e-6\n",
    "ADAM_EPSILON = 1e-5\n",
    "SAVE_EPSILON = 1e-4\n",
    "PREPROCESS = True\n",
    "NEW_LABELS = True\n",
    "NUM_CLASSES = len(LABEL_CHANNELS) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(image_input, keep_prob, weights_file, num_classes):   \n",
    "    \"\"\"\n",
    "    Builds custom network based on VGG:\n",
    "    1) First 13 layers (10 conv, 3 dialated conv) use pre-trained weights\n",
    "    2) Remaining layers are new, resized with reduced depth than original VGG layers\n",
    "    3) Final layer is added with scaled (0.01) output from conv3 layer\n",
    "    \n",
    "    \n",
    "    :param image_input: image input tensor\n",
    "    :param keep_prob: placeholder for drop out\n",
    "    :weights_file: path to pre-trained weights file\n",
    "    :num_classes: number of classes for final output\n",
    "    :return: logits, prediction, one_hot\n",
    "    \"\"\"    \n",
    "    def conv_layer(name, input_layer, weights):\n",
    "        \"\"\"\n",
    "        Builds conv layers from pre-trained weights\n",
    "        Adopted from: \n",
    "            1) https://github.com/fyu/dilation\n",
    "            2) https://github.com/ndrplz/dilation-tensorflow\n",
    "            \n",
    "        :param name: layer name\n",
    "        :param input_layer: input tensor\n",
    "        :param weights: pre-trained weights dictionary\n",
    "        :return: conv layer tensor\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            kernel = tf.Variable(initial_value=weights[name[:7] + '/kernel:0'], name='kernel')\n",
    "            bias = tf.Variable(initial_value=weights[name[:7] + '/bias:0'], name='bias')\n",
    "            conv = tf.nn.conv2d(input_layer, kernel, strides=[1,1,1,1], padding='SAME', name='conv')\n",
    "            out = tf.nn.bias_add(conv, bias, name='bias_add')\n",
    "            out = tf.nn.relu(out, name='relu')\n",
    "            return out\n",
    "\n",
    "    def aconv_layer(name, input_layer, weights, rate):\n",
    "        \"\"\"\n",
    "        Builds atrous/dilated conv layers from pre-trained weights\n",
    "        Adopted from: \n",
    "            1) https://github.com/fyu/dilation\n",
    "            2) https://github.com/ndrplz/dilation-tensorflow\n",
    "            \n",
    "        :param name: layer name\n",
    "        :param input_layer: input tensor\n",
    "        :param weights: pre-trained weights dictionary\n",
    "        :param rate: rate of dilation\n",
    "        :return: atrous/dilated conv layer tensor\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            kernel = tf.Variable(initial_value=weights[name[1:8] + '/kernel:0'], name='kernel')\n",
    "            bias = tf.Variable(initial_value=weights[name[1:8] + '/bias:0'], name='bias')\n",
    "            aconv = tf.nn.atrous_conv2d(input_layer, kernel, rate, padding='SAME', name='aconv')\n",
    "            out = tf.nn.bias_add(aconv, bias, name='bias_add')\n",
    "            out = tf.nn.relu(out, name='relu')\n",
    "            return out\n",
    "\n",
    "    def max_pool(name, input_layer):\n",
    "        \"\"\"\n",
    "        Builds maxpooling layer with VGG default values\n",
    "        \n",
    "        :param name: layer name\n",
    "        :param input_layer: input tensor\n",
    "        :return: maxpooling layer tensor\n",
    "        \"\"\"\n",
    "        return tf.layers.max_pooling2d(input_layer, pool_size=(2,2), strides=(2,2), padding='SAME', name=name)\n",
    "\n",
    "    with open(weights_file, 'rb') as f:\n",
    "        pre_w = pickle.load(f)\n",
    "    \n",
    "    conv1_1 = conv_layer('conv1_1_64', image_input, pre_w)\n",
    "    conv1_2 = conv_layer('conv1_2_64', conv1_1, pre_w)\n",
    "    pool1 = max_pool('pool1', conv1_2)\n",
    "    \n",
    "    conv2_1 = conv_layer('conv2_1_128', pool1, pre_w)\n",
    "    conv2_2 = conv_layer('conv2_2_128', conv2_1, pre_w)\n",
    "    pool2 = max_pool('pool2', conv2_2)\n",
    "    \n",
    "    conv3_1 = conv_layer('conv3_1_256', pool2, pre_w)\n",
    "    conv3_2 = conv_layer('conv3_2_256', conv3_1, pre_w)\n",
    "    conv3_3 = conv_layer('conv3_3_256', conv3_2, pre_w)\n",
    "    pool3 = max_pool('pool3', conv3_3)\n",
    "    \n",
    "    conv4_1 = conv_layer('conv4_1_512', pool3, pre_w)\n",
    "    conv4_2 = conv_layer('conv4_2_512', conv4_1, pre_w)\n",
    "    conv4_3 = conv_layer('conv4_3_512', conv4_2, pre_w)\n",
    "    \n",
    "    # Dilated convolutions, rate = 2\n",
    "    dconv5_1 = aconv_layer('dconv5_1_512', conv4_3, pre_w, 2)\n",
    "    dconv5_2 = aconv_layer('dconv5_2_512', dconv5_1, pre_w, 2)\n",
    "    dconv5_3 = aconv_layer('dconv5_3_512', dconv5_2, pre_w, 2)\n",
    "    \n",
    "    # Dialated convolition, rate = 4\n",
    "    dconv6_1 = tf.layers.conv2d(dconv5_3, 512, kernel_size=7, strides=1, padding='SAME', \n",
    "                           name='dconv6_1_512',\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG),\n",
    "                           dilation_rate=4, activation=tf.nn.relu)\n",
    "    \n",
    "    drop1 = tf.nn.dropout(dconv6_1, keep_prob, name='drop1') \n",
    "    \n",
    "    conv7_1 = tf.layers.conv2d(drop1, 512, kernel_size=1, strides=1, padding='SAME', \n",
    "                           name='conv7_1_512',\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG),\n",
    "                           activation=tf.nn.relu)\n",
    "    \n",
    "    drop2 = tf.nn.dropout(conv7_1, keep_prob, name='drop2')\n",
    "\n",
    "    \n",
    "    conv8_1 = tf.layers.conv2d(drop2, num_classes, kernel_size=1, strides=1, padding='SAME', \n",
    "                            name='conv8_1_nc',\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG))   \n",
    "    \n",
    "    # Transposed convolution with a factor of 2\n",
    "    tconv9_1 = tf.layers.conv2d_transpose(conv8_1, num_classes, 4, 2, padding='SAME', \n",
    "                            name='tconv9_1_nc',\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG)) \n",
    "    \n",
    "    conv3_4 = tf.layers.conv2d(conv3_3, num_classes, kernel_size=1, strides=1, padding='SAME', \n",
    "                            name='conv3_4_nc',\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG))\n",
    "    \n",
    "    conv3_4_scale = tf.multiply(conv3_4, 0.01)\n",
    "    \n",
    "    add_conv3_conv9 = tf.add(conv3_4_scale, tconv9_1, name='add_conv3_conv9')\n",
    "    \n",
    "    # Transposed convolution with a factor of 4\n",
    "    tconv10_1 = tf.layers.conv2d_transpose(add_conv3_conv9, num_classes, 8, 4, \n",
    "                            padding='SAME', name='tconv10_1_nc',\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG)) \n",
    "    \n",
    "    with tf.name_scope('output'):\n",
    "        softmax = tf.nn.softmax(tconv10_1, name='softmax')\n",
    "        prediction = tf.argmax(softmax, axis=3, name='prediction')\n",
    "        one_hot = tf.one_hot(prediction, depth=num_classes, dtype=tf.uint8, name='one_hot')\n",
    "        \n",
    "    return tconv10_1, prediction, one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(logits, labels, l_rate, adam_eps, weights=None):\n",
    "    \"\"\"\n",
    "    Creates optimization and loss functions:\n",
    "        1) Uses Adam optimizer\n",
    "        2) Loss based on weighted cross entropy + regularization loss\n",
    "    \n",
    "    :param logits: logits tensor from network()\n",
    "    :param labels: placeholder for training labels\n",
    "    :param l_rate: placeholder for learning rate value\n",
    "    :param adam_eps: placeholder for Adam epsilon values\n",
    "    :param weights: placeholder for weights, if None no weighting is applied\n",
    "    :return: optimizer, total_loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope('optimize'):\n",
    "        logits = tf.reshape(logits, (-1, NUM_CLASSES))\n",
    "        labels = tf.to_float(tf.reshape(labels, (-1, NUM_CLASSES)))\n",
    "        \n",
    "        if weights is not None:\n",
    "            softmax = tf.nn.softmax(logits) + EPSILON\n",
    "            cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax), weights),\n",
    "                                           reduction_indices=[1])\n",
    "        else:\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "            \n",
    "        cross_entropy_loss = tf.reduce_mean(cross_entropy,\n",
    "                                            name='xent_mean_loss')\n",
    "                                        \n",
    "        reg_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES),\n",
    "                            name='reg_loss')\n",
    "        total_loss = tf.add_n([cross_entropy_loss, reg_loss], name='total_loss')\n",
    "        \n",
    "        #optimizer = tf.train.MomentumOptimizer(l_rate, m_rate).minimize(total_loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=l_rate, epsilon=adam_eps).minimize(total_loss)\n",
    "        #optimizer = tf.train.RMSPropOptimizer(learning_rate=l_rate).minimize(total_loss)\n",
    "        \n",
    "    return optimizer, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MODEL_NAME: {MODEL_NAME}')\n",
    "print(f'MODEL_SAVE_VER: {MODEL_SAVE_VER}')\n",
    "print(f'TRAIN_DIR: {TRAIN_DIR}')\n",
    "print(f'TEST_DIR: {TEST_DIR}')\n",
    "print(f'SAVE_DIR: {SAVE_DIR}')\n",
    "print(f'WEIGHTS_FILE: {WEIGHTS_FILE}')\n",
    "\n",
    "print(f'SHUFFLE_PER_EPOCH: {SHUFFLE_PER_EPOCH}')\n",
    "print(f'BATCH_SIZE: {BATCH_SIZE}')\n",
    "print(f'L2_REG: {L2_REG}')\n",
    "print(f'STD_DEV: {STD_DEV}')\n",
    "print(f'LEARNING_RATE: {LEARNING_RATE}')\n",
    "print(f'MOMENTUM: {MOMENTUM}')\n",
    "print(f'KEEP_PROB: {KEEP_PROB}')\n",
    "print(f'EPSILON: {EPSILON}')\n",
    "print(f'ADAM_EPSILON: {ADAM_EPSILON}')\n",
    "\n",
    "print(f'TRIM: {TRIM}')\n",
    "print(f'TRIM_IND: {TRIM_IND}')\n",
    "print(f'FLIP: {FLIP}')\n",
    "print(f'RESHAPE: {RESHAPE}')\n",
    "print(f'NEW_SHAPE: {NEW_SHAPE}')\n",
    "print(f'PREPROCESS: {PREPROCESS}')\n",
    "\n",
    "print(f'NEW_LABELS: {NEW_LABELS}')\n",
    "print(f'LABEL_CHANNELS: {LABEL_CHANNELS}')\n",
    "print(f'CHANNEL_NAMES: {CHANNEL_NAMES}')\n",
    "print(f'LOSS_WEIGHTS: {LOSS_WEIGHTS}')\n",
    "\n",
    "get_train_batch = hf.train_batch_gen(TRAIN_DIR, LABEL_CHANNELS, reshape=RESHAPE, new_shape=NEW_SHAPE, \n",
    "                                     preprocess=PREPROCESS, new_labels=NEW_LABELS, \n",
    "                                     trim=TRIM, trim_ind=TRIM_IND)\n",
    "get_test_batch, revert_trim_reshape = hf.test_batch_gen(TEST_DIR, LABEL_CHANNELS, reshape=RESHAPE, \n",
    "                                      new_shape=NEW_SHAPE, preprocess=PREPROCESS, new_labels=NEW_LABELS,\n",
    "                                      trim=TRIM, trim_ind=TRIM_IND)\n",
    "\n",
    "# Test images are loaded into memory to reduce training time\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_names = []\n",
    "for images, labels, names in get_test_batch(1):\n",
    "    test_images.append(images)\n",
    "    test_labels.append(labels)\n",
    "    test_names += names\n",
    "\n",
    "test_images = np.array(test_images, dtype=np.uint8)\n",
    "test_images = test_images.reshape(-1, *test_images.shape[2:])\n",
    "test_labels = np.array(test_labels, dtype=np.uint8)\n",
    "test_labels = test_labels.reshape(-1, *test_labels.shape[2:])   \n",
    "print(f'test_images.shape: {test_images.shape}')\n",
    "print(f'test_labels.shape: {test_labels.shape}')\n",
    "\n",
    "flat_labels_size = reduce(lambda x, y: x*y, test_labels.shape[:-1])\n",
    "image_org_shape = (test_labels.shape[1], test_labels.shape[2])\n",
    "flat_offset = BATCH_SIZE*image_org_shape[0]*image_org_shape[1]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    image_input = tf.placeholder(tf.float32, (None, None, None, 3), name='image_input')\n",
    "    label_input = tf.placeholder(tf.int32, [None, None, None, NUM_CLASSES], name='label_input')\n",
    "    loss_weights = tf.placeholder(tf.float32, (None), name='loss_weights')\n",
    "    keep_prob = tf.placeholder_with_default(tf.constant(1.0, dtype=tf.float32), shape=(), name='keep_prob')\n",
    "    l_rate = tf.placeholder(tf.float32, name='l_rate')\n",
    "    adam_eps = tf.placeholder(tf.float32, name='adam_eps')\n",
    "    \n",
    "    logits, prediction, one_hot = network(image_input, keep_prob, WEIGHTS_FILE, NUM_CLASSES)\n",
    "    opt, total_loss = optimize(logits, label_input, l_rate, adam_eps, loss_weights)\n",
    "    \n",
    "    fscore_avg = 0.0\n",
    "    best_fscore = 0.0\n",
    "    best_loss = 9999\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        print(f'\\nTraining epoch: {epoch+1}/{EPOCHS}')\n",
    "        \n",
    "        \n",
    "        for train_image_batch, train_label_batch, _ in get_train_batch(BATCH_SIZE):\n",
    "            \n",
    "            if FLIP:\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    # horizontal flip\n",
    "                    train_image_batch = np.flip(train_image_batch, axis=2)\n",
    "                    train_label_batch = np.flip(train_label_batch, axis=2)\n",
    "                \n",
    "            _, loss = sess.run([opt, total_loss],\n",
    "                               feed_dict = {image_input: train_image_batch,\n",
    "                                            label_input: train_label_batch,\n",
    "                                            keep_prob: KEEP_PROB,\n",
    "                                            l_rate: LEARNING_RATE,\n",
    "                                            adam_eps: ADAM_EPSILON,\n",
    "                                            loss_weights: LOSS_WEIGHTS})\n",
    "        print(f'Training time: {(time.time() - start_time):#0.3f}s, loss: {loss:#0.5f}') \n",
    "        \n",
    "        sess_time = 0\n",
    "        total_preds = np.empty((flat_labels_size,), dtype=np.uint8)\n",
    "        total_labels = np.empty((flat_labels_size,), dtype=np.uint8)\n",
    "        for offset in range(0, len(test_images), BATCH_SIZE):\n",
    "            pred_time = time.time()\n",
    "            test_image_batch = test_images[offset:offset+BATCH_SIZE]\n",
    "            test_label_batch = test_labels[offset:offset+BATCH_SIZE]            \n",
    "            preds = sess.run(prediction, feed_dict = {image_input: test_image_batch})\n",
    "            \n",
    "            preds = revert_trim_reshape(preds)\n",
    "            sess_time += time.time() - pred_time\n",
    "            \n",
    "            preds_result = np.array(preds, dtype=np.uint8).reshape(-1)\n",
    "            labels_result = test_label_batch.argmax(axis=3).reshape(-1)\n",
    "            \n",
    "            batch_offset = len(test_label_batch)*image_org_shape[0]*image_org_shape[1]\n",
    "            i = int(offset/BATCH_SIZE)\n",
    "            total_preds[i*flat_offset:i*flat_offset+batch_offset] = preds_result\n",
    "            total_labels[i*flat_offset:i*flat_offset+batch_offset] = labels_result\n",
    "            \n",
    "        print(f'Prediction session time: {sess_time:#0.3f}s')\n",
    "        eval_start_time = time.time()\n",
    "        metrics = precision_recall_fscore_support(total_labels, total_preds)\n",
    "        print(f'Evaluation time: {(time.time() - eval_start_time):#0.3f}s')\n",
    "        del total_preds\n",
    "        del total_labels \n",
    "        \n",
    "        str_title     = f'             '\n",
    "        str_recall    = f'Recall:    '\n",
    "        str_precision = f'Precision: '\n",
    "        str_f1        = f'F1 score:  '\n",
    "        str_support   = f'Support:   '\n",
    "        for i, val in enumerate(metrics[0]):\n",
    "            str_title += f'{CHANNEL_NAMES[i]:10}'\n",
    "            str_recall += f'{val:#10.6f}'\n",
    "            str_precision += f'{metrics[1][i]:#10.6f}'\n",
    "            str_f1 += f'{metrics[2][i]:#10.6f}'\n",
    "            str_support += f'{metrics[3][i]:10}'\n",
    "        print(str_title)\n",
    "        print(str_recall)\n",
    "        print(str_precision)\n",
    "        print(str_f1)\n",
    "        print(str_support)\n",
    "        \n",
    "        fscore_avg = np.mean(np.array(metrics[2]))\n",
    "        print(f'fscore_avg: {fscore_avg:#0.5f}')\n",
    "        print(f'Total time: {time.time()-start_time:#0.3f}s')\n",
    "        \n",
    "        if fscore_avg - best_fscore > SAVE_EPSILON:\n",
    "            best_fscore = fscore_avg\n",
    "            saver.save(sess, os.path.join(SAVE_DIR, 'score', MODEL_NAME + '.ckpt'))  \n",
    "            print('*************** MODEL SAVED ON FSCORE ***************')\n",
    "        elif best_loss - loss > SAVE_EPSILON:\n",
    "            best_loss = loss\n",
    "            saver.save(sess, os.path.join(SAVE_DIR, 'loss', MODEL_NAME + '.ckpt'))  \n",
    "            print('*** model saved on loss ***')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
